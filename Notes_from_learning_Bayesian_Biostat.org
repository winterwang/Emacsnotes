#+TITLE:    Bayesian Biostatistics Learning note
#+AUTHOR:   [[https://wangcc.me][Chaochen Wang]]
#+EMAIL:    chaochen@wangcc.me
#+OPTIONS: d:(not "LOGBOOK") date:t e:t email:t f:t inline:t num:t
#+OPTIONS: timestamp:t title:t toc:t todo:t |:t


* Chapter 2 Bayes theorem: computing the posterior distribution

** The binary version of Bayes theorem

$$
p(\theta = 1 | y = 1) = 
\frac{p(y = 1 | \theta = 1)\times p(\theta=1)}
{p(y = 1 | \theta = 1)\times p(\theta = 1) +
p(y = 1 | \theta  = 0)\times p(\theta = 0)}
$$

In the above exporession, 

- $p(\theta = 1)(p(\theta = 0))$ represents the prevalence (1 -
  prevalence) of a disease. But in general it is called the /prior/
  probability that $\theta = 1 (\theta = 0)$.

- The probability $p(y = 1 | \theta = 1)$ describes the probability
  that a positive test is obtained, given that $\theta = 1$. It is the
  **likelihood** for $\theta = 1$ with a positive test.

- Finally, $p(\theta = 1 | y = 1)$ is the probability that an
  individual is diseased given observing a positive test. This
  probability is called the /posterior/ probability that is derived
  from combining the prior information and the observed data.

$$
p(\theta | y) = \frac{p(y | \theta)p(\theta)}{p(y)}
$$

** Probability in a Bayesian context 

1) For an "honest" coin, the classical probablity that a coin will
   show heads when tossed up is 0.5. This **probability** has a
   /long-run frequency meaning/. **"objective"**
2) Suppose that you don't believe that there exist honest coins then
   you might give it a probability of 0.6. This kind of probability
   expresses your /personal belief/ and is typically an example of
   /Bayesian probability/. **"subjective"**
** Bayes theorem -- the categorical version

Suppose a subject can belong to $K > 2$ diagnostic classes
corresponding to $K$ values for $\theta: \theta_1, \theta_2, \dots,
\theta_K$. Assuming that $y$ can take on $L$ different values: $y_1,
\dots, y_L$,then Bayes theorem generalizes to: 

$$
p(\theta_k | y) = \frac{p(y | \theta_k)p(\theta_k)}{\sum_{m = 1}^Kp(y | \theta_m)p(\theta_m)}
$$

Where $y$ stands for one of the possible values in $\{y_1, \dots, y_L\}$.

** Bayes theorem --  the continuous version


$$
p(\theta|y) = \frac{L(\theta | y)p(\theta)}{p(y)} = \frac{L(\theta | y)p(\theta)}{\int L(\theta|y)p(\theta)d\theta}
$$

- Prior opinion about the parameter is expressed as a distribution: $p(\theta)$.
- Observed data: $L(\theta | y)$
