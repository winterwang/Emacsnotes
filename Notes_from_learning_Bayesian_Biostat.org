#+TITLE:    Bayesian Biostatistics Learning note
#+AUTHOR:   [[https://wangcc.me][Chaochen Wang]]
#+EMAIL:    chaochen@wangcc.me
#+OPTIONS: d:(not "LOGBOOK") date:t e:t email:t f:t inline:t num:t
#+OPTIONS: timestamp:t title:t toc:t todo:t |:t


* Chapter 2 Bayes theorem: computing the posterior distribution

** The binary version of Bayes theorem

$$
p(\theta = 1 | y = 1) = 
\frac{p(y = 1 | \theta = 1)\times p(\theta=1)}
{p(y = 1 | \theta = 1)\times p(\theta = 1) +
p(y = 1 | \theta  = 0)\times p(\theta = 0)}
$$

In the above exporession, 

- $p(\theta = 1)(p(\theta = 0))$ represents the prevalence (1 -  prevalence) of a disease. But in general it is called the /prior/ probability that $\theta = 1 (\theta = 0)$.

- The probability $p(y = 1 | \theta = 1)$ describes the probability that a positive test is obtained, given that $\theta = 1$. It is the **likelihood** for $\theta = 1$ with a positive test.

- Finally, $p(\theta = 1 | y = 1)$ is the probability that an individual is diseased given observing a positive test. This
  probability is called the /posterior/ probability that is derived from combining the prior information and the observed data.

$$
p(\theta | y) = \frac{p(y | \theta)p(\theta)}{p(y)}
$$




** Probability in a Bayesian context 

1) For an "honest" coin, the classical probablity that a coin will show heads when tossed up is 0.5. This **probability** has a /long-run frequency meaning/. **"objective"**
2) Suppose that you don't believe that there exist honest coins the you might give it a probability of 0.6. This kind of probability expresses your /personal belief/ and is typically an example of /Bayesian probability/. **"subjective"**

- We can be uncertain about basically all things in life, but typically these uncertainties exhibit a great range, may change in time and vary with the individual.

- Lindley(2006) argues that probability is a totally different concept from, say, distance. The distance between two points is the same for all of us, but that is not the case with probability of events since it depends on the person looking at the world. 

** Bayes theorem -- the categorical version

Suppose a subject can belong to $K > 2$ diagnostic classes corresponding to $K$ values for $\theta: \theta_1, \theta_2, \dots, \theta_K$. Assuming that $y$ can take on $L$ different values: $y_1, \dots, y_L$,then Bayes theorem generalizes to: 

$$
p(\theta_k | y) = \frac{p(y | \theta_k)p(\theta_k)}{\sum_{m = 1}^Kp(y | \theta_m)p(\theta_m)}
$$

Where $y$ stands for one of the possible values in $\{y_1, \dots, y_L\}$.

** Bayes theorem --  the continuous version

$$
p(\theta|y) = \frac{L(\theta | y)p(\theta)}{p(y)} = \frac{L(\theta | y)p(\theta)}{\int L(\theta|y)p(\theta)d\theta}
$$

- Prior opinion about the parameter is expressed as a distribution: $p(\theta)$ ;
- Observed data: $L(\theta | y)$ ; 
- The denominator $\int L(\theta|y)p(\theta)d\theta$ ensures that $p(\theta | y)$ is indeed a distribution, and is called /averaged likelihood/.

Therefore, when the prior opinion about the parameter as a distribution is combined with the observed data, the opinion will be updated and is expressed by the posterior distribution. This expression also tells us that when both the prior and the likelihood support a particular $\theta$, then this $\theta$ also supported a posteriori. But if $\theta$ is not supported by either the prior distribution or the likelihood or both, then neither $\theta$ is supported by a posteriori. 

$$
p(\theta | y) \propto L(\theta | y)p(\theta)
$$

- The denominator is depending only on the observed data which is assumed to be fixed in a Bayesian context. 
- The denominator in the above expression is called the /averaged likelihood/ because it is the weighted average of $L(\theta|y)$ over the possible values of $\theta$ with a weight function given by the prior distribution $p(\theta)$.
 
